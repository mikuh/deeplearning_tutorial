# Attention机制

人类在阅读或者观察事物的时候，往往是有针对性的，他不可能一下子记住所有的东西，而是会有选择的关注到一些重点，基于这种直觉，我们将人类学习和理解事物的特点用到深度学习里面，就有了Attention机制。

我最早看到Attention机制的时候大概是在Google的神经翻译模型里面，这东西对模型表现有很好的效果，再后来2017年的时候有一篇轰动一时的论文《Attention is all you need》把attention弄到了一个更加重要的位置，虽然一开始并没有造成多大的冲击，但是在2018年之后，attention机制逐渐开始一统江湖。

于是，我们就从最早的最流行的编码-解码模型来认识什么是attention,其本质上就是加权平均。

![](../images/a1.png)

直观的说，Encoder-Decoder框架是一个序列到另一个序列的转换，它的输入和输出是一个句子对(source, target)，即目标是通给定句子$source = (x_1, x_2,...,x_m)$，通过encoder-decoder转换为目标$target=(y_1, y_2, ...,y_n)$。

在没有attention的情况下，这个转换应该是这样的：

首先将source通过一个编码器的非线性变化$f$转换成一个中间语义状态C：
$$
C=f(x_1,x_2,...,x_m)
$$
然后进入解码器，预测target的一个一个的词，此时第$t​$ 个输出$y_t​$除了和编码器的$C​$ 有关，还和前面的$y_1，y_2,...y_{t-1}​$ 有关，我们将这个非线性过程用$g​$表示:
$$
y_t = g(C;y_1,y_2,...,y_{t-1})
$$
也就是说，所有的$y_t$ 都和同样的 $C​$ 有关